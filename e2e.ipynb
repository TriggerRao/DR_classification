{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9riOOnSRT5ta",
        "outputId": "cad6a2d5-ee1b-4fb1-e7fb-6e8ddbf8487f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit_keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.11.4)\n",
            "Collecting validators (from vit_keras)\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.25.2)\n",
            "Installing collected packages: validators, vit_keras\n",
            "Successfully installed validators-0.28.1 vit_keras-0.1.2\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n",
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Installing collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.9.0.80\n"
          ]
        }
      ],
      "source": [
        "!pip install vit_keras\n",
        "!pip install tensorflow-addons\n",
        "!pip install openpyxl\n",
        "!pip install opencv-python opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DniTPjmYRfN1",
        "outputId": "11887627-590f-40a6-a4aa-8de71d225e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DRnm6asTLbV",
        "outputId": "9632a9d7-3fea-4ea8-a57c-1b6fb154775d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from vit_keras import vit\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U24WxFo1TIFr",
        "outputId": "d3625ce7-be90-4127-cf1b-358836078415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_32_imagenet21k+imagenet2012.npz\n",
            "353253686/353253686 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 7, 7\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vit_model = vit.vit_b32(\n",
        "    image_size=(224,224),\n",
        "    activation='softmax',\n",
        "    pretrained=True,\n",
        "    include_top=False,\n",
        "    pretrained_top=False,\n",
        "    classes=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84TdOfSmdd2M",
        "outputId": "18a2a863-2ac6-4992-9bec-58de698a99ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1856\n",
            "175\n",
            "115\n",
            "23\n",
            "29\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists to hold images, numerical features, and labels\n",
        "images, numerical_features, labels = [], [], []\n",
        "\n",
        "numerical_features_df = pd.read_excel(\"/content/drive/MyDrive/Baseline_DR/DR_Baseline_FINAL.xlsx\")\n",
        "occurances0 = (numerical_features_df[\"os\"]==0).sum() + (numerical_features_df[\"od\"]==0).sum()\n",
        "occurances1 = (numerical_features_df[\"os\"]==1).sum() + (numerical_features_df[\"od\"]==1).sum()\n",
        "occurances2 = (numerical_features_df[\"os\"]==2).sum() + (numerical_features_df[\"od\"]==2).sum()\n",
        "occurances3 = (numerical_features_df[\"os\"]==3).sum() + (numerical_features_df[\"od\"]==3).sum()\n",
        "occurances4 = (numerical_features_df[\"os\"]==4).sum() + (numerical_features_df[\"od\"]==4).sum()\n",
        "print(occurances0)\n",
        "print(occurances1)\n",
        "print(occurances2)\n",
        "print(occurances3)\n",
        "print(occurances4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5uG5V4lrlUG"
      },
      "outputs": [],
      "source": [
        "def extract_capital_alphabets_and_replace_underscores(input_string):\n",
        "\n",
        "    initial_chars = \"D R P E \"\n",
        "    if input_string.startswith(initial_chars):\n",
        "        input_string = input_string[len(initial_chars):]\n",
        "    # Use regular expression to match capital alphabets and underscores\n",
        "    pattern = r'[A-Z_ ]'\n",
        "\n",
        "    # Find all matches in the input string\n",
        "    matches = re.findall(pattern, input_string)\n",
        "\n",
        "    # print(matches)\n",
        "\n",
        "    # Concatenate the matches into a single string\n",
        "    result_string = ''.join(matches)\n",
        "\n",
        "    # Replace underscores with spaces\n",
        "    result_string = result_string.replace('_', ' ')\n",
        "\n",
        "    result_string = result_string.strip()\n",
        "\n",
        "    return result_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMQZBx5ode2i",
        "outputId": "53fa06af-d404-460e-b594-aac6f7a73206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2198\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "\n",
        "# Load images and labels\n",
        "for label in range(5):  # Assuming labels 0 to 4\n",
        "    folder_path = os.path.join(\"/content/drive/MyDrive/Baseline_DR\", str(label))\n",
        "    for filename in os.listdir(folder_path):\n",
        "        count+=1\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            name_key = extract_capital_alphabets_and_replace_underscores(filename)  # Extract name key using the defined function\n",
        "            # print(filename)\n",
        "            # print(name_key)\n",
        "            if name_key:\n",
        "                num_feature = numerical_features_df[numerical_features_df[\"names\"].str.upper() == name_key]\n",
        "                # if not num_feature.empty:\n",
        "                #     for index, row in num_feature.iterrows():\n",
        "                #         if label == row[\"DR multiclass\"]:\n",
        "                #             count += 1\n",
        "                if not num_feature.empty:\n",
        "                    img_path = os.path.join(folder_path, filename)\n",
        "                    img = cv2.imread(img_path)\n",
        "                    img = cv2.resize(img, (224, 224))  # Assuming image_input_shape is (224, 224, 3)\n",
        "                    img = img / 255.0  # Normalize pixel values\n",
        "\n",
        "                    images.append(img)\n",
        "                    numerical_features.append(num_feature.drop(columns=[\"names\", \"DR\", \"DR multiclass\", \"od\", \"os\"]).values[0])\n",
        "                    labels.append(label)\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEPCIy2UnrFT",
        "outputId": "33f3e180-3e22-4009-c783-5698909b157d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2116\n",
            "2116\n",
            "2116\n"
          ]
        }
      ],
      "source": [
        "print(len(images))\n",
        "print(len(numerical_features))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOCZFeVe7uRk"
      },
      "outputs": [],
      "source": [
        "###DEFINING THE TRANSFORMS\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an instance of ImageDataGenerator with desired augmentation parameters\n",
        "datagen1 = ImageDataGenerator(\n",
        "    rotation_range=15,  # Rotate images by up to 15 degrees\n",
        "    zoom_range=0.1,  # Zoom images by up to 10%\n",
        "    brightness_range=[0.8, 1.2],  # Adjust brightness by up to 20%\n",
        "    horizontal_flip=True,  # Flip images horizontally\n",
        "    vertical_flip=True,  # Flip images vertically\n",
        "    fill_mode='nearest'  # Fill in missing pixels with the nearest value\n",
        ")\n",
        "\n",
        "# Create an instance of ImageDataGenerator with desired augmentation parameters\n",
        "datagen2 = ImageDataGenerator(\n",
        "    rotation_range=15,  # Rotate images by up to 15 degrees\n",
        "    zoom_range=0.1,  # Zoom images by up to 10%\n",
        "    brightness_range=[0.8, 1.2],  # Adjust brightness by up to 20%\n",
        "    horizontal_flip=True,  # Flip images horizontally\n",
        "    vertical_flip=False,  # Flip images vertically\n",
        "    fill_mode='nearest'  # Fill in missing pixels with the nearest value\n",
        ")\n",
        "\n",
        "# Create an instance of ImageDataGenerator with desired augmentation parameters\n",
        "datagen3 = ImageDataGenerator(\n",
        "    rotation_range=15,  # Rotate images by up to 15 degrees\n",
        "    zoom_range=0.1,  # Zoom images by up to 10%\n",
        "    brightness_range=[0.8, 1.2],  # Adjust brightness by up to 20%\n",
        "    horizontal_flip=True,  # Flip images horizontally\n",
        "    vertical_flip=False,  # Flip images vertically\n",
        "    fill_mode='nearest'  # Fill in missing pixels with the nearest value\n",
        ")\n",
        "\n",
        "# Create an instance of ImageDataGenerator with desired augmentation parameters\n",
        "datagen4 = ImageDataGenerator(\n",
        "    rotation_range=15,  # Rotate images by up to 15 degrees\n",
        "    zoom_range=0.1,  # Zoom images by up to 10%\n",
        "    brightness_range=[0.8, 1.2],  # Adjust brightness by up to 20%\n",
        "    horizontal_flip=True,  # Flip images horizontally\n",
        "    vertical_flip=False,  # Flip images vertically\n",
        "    fill_mode='nearest'  # Fill in missing pixels with the nearest value\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpGH0lGdRHTc",
        "outputId": "eb92416c-f0d4-41fc-83bc-7d20ade4708a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2116\n",
            "[1740, 198, 124, 22, 32]\n"
          ]
        }
      ],
      "source": [
        "counts = [0] * 5\n",
        "print(len(labels))\n",
        "for label in labels:\n",
        "  counts[label]+=1\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe-SLsp9RHTd"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "# Iterate through your images and apply augmentation\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "augmented_nfs = []\n",
        "\n",
        "\n",
        "for image, label, nf in zip(images, labels, numerical_features):\n",
        "    # Reshape the image to (1, height, width, channels) for compatibility with ImageDataGenerator\n",
        "    image = image.reshape((1,) + image.shape)\n",
        "    count = 0\n",
        "    if label == 0:\n",
        "        count = 1\n",
        "    else:\n",
        "        count = 2*counts[0]/counts[label]\n",
        "        count = math.floor(count) - 1\n",
        "\n",
        "    while count > 0:  # Use a while loop to apply augmentation 'count' times\n",
        "        if count % 4 == 0:\n",
        "            # Generate augmented images from the original image\n",
        "            augmented_data = datagen1.flow(image, batch_size=1)\n",
        "        elif (count - 1) % 4 == 0:\n",
        "            # Generate augmented images from the original image\n",
        "            augmented_data = datagen2.flow(image, batch_size=1)\n",
        "        elif (count - 2) % 4 == 0:\n",
        "            # Generate augmented images from the original image\n",
        "            augmented_data = datagen3.flow(image, batch_size=1)\n",
        "        else:\n",
        "            # Generate augmented images from the original image\n",
        "            augmented_data = datagen4.flow(image, batch_size=1)\n",
        "\n",
        "        # Extract augmented image and label\n",
        "        augmented_image = next(augmented_data)[0].astype('uint8')  # Convert back to uint8 format\n",
        "        augmented_label = label  # Labels remain unchanged in augmentation\n",
        "        augmented_nf = nf\n",
        "\n",
        "        augmented_images.append(augmented_image)\n",
        "        augmented_labels.append(augmented_label)\n",
        "        augmented_nfs.append(augmented_nf)\n",
        "        count -= 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfO4HKeYRHTe",
        "outputId": "4cc0745a-ff28-47ad-962f-9d958e11f4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1740, 3168, 3348, 3454, 3424]\n"
          ]
        }
      ],
      "source": [
        "label_count = [0] * 5\n",
        "for label in augmented_labels:\n",
        "    label_count[label]+=1\n",
        "print(label_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtEMiUyPRHTe"
      },
      "outputs": [],
      "source": [
        "images += augmented_images\n",
        "labels += augmented_labels\n",
        "numerical_features += augmented_nfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrmLK0C9RHTe",
        "outputId": "66120a8e-f778-4350-cbb7-9ad97be1ef4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3480, 3366, 3472, 3476, 3456]\n",
            "17250\n",
            "17250\n"
          ]
        }
      ],
      "source": [
        "label_count = [0] * 5\n",
        "for label in labels:\n",
        "    label_count[label]+=1\n",
        "print(label_count)\n",
        "\n",
        "print(len(images))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKf0IMIJTJgm",
        "outputId": "a72abde9-c1c9-46f9-a417-e26da3e5422d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'keras.src.engine.keras_tensor.KerasTensor'>\n",
            "(None, 768)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " image_input (InputLayer)    [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " vit-b32 (Functional)        (None, 768)                  8745523   ['image_input[0][0]']         \n",
            "                                                          2                                       \n",
            "                                                                                                  \n",
            " num_features_input (InputL  [(None, 47)]                 0         []                            \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 815)                  0         ['vit-b32[0][0]',             \n",
            "                                                                     'num_features_input[0][0]']  \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 818)                  667488    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 818)                  0         ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 256)                  209664    ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 5)                    1285      ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 88333669 (336.97 MB)\n",
            "Trainable params: 88333669 (336.97 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "image_input_shape = (224, 224, 3)  # Assuming RGB images\n",
        "num_features = 47\n",
        "num_classes = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# Define inputs for image and numerical features\n",
        "image_input = layers.Input(shape=image_input_shape, name=\"image_input\")\n",
        "num_features_input = layers.Input(shape=(num_features,), name=\"num_features_input\")\n",
        "\n",
        "# Obtain image embeddings using ViT\n",
        "image_embeddings = vit_model(image_input)\n",
        "\n",
        "print(type(image_embeddings))\n",
        "print(image_embeddings.shape) #768\n",
        "\n",
        "\n",
        "# Concatenate image embeddings with numerical features\n",
        "concatenated_features = layers.Concatenate()([image_embeddings, num_features_input])\n",
        "\n",
        "# Add fully connected layers for classification\n",
        "x = layers.Dense(818, activation=\"relu\")(concatenated_features)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(256, activation=\"relu\")(x)\n",
        "output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[image_input, num_features_input], outputs=output)\n",
        "\n",
        "# Create an Adam optimizer with the specified learning rate\n",
        "adam_optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=adam_optimizer,\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjwrSaLjRlys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a16fae-ac32-4691-e162-7a8769ca8fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17250, 47)\n"
          ]
        }
      ],
      "source": [
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "numerical_features = np.array(numerical_features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(numerical_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2t_b9b0RHTg",
        "outputId": "3d3a6beb-932f-45db-f25b-b88d3c8cd5aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "432/432 [==============================] - 802s 2s/step - loss: 4.0669 - accuracy: 0.3896 - val_loss: 1.1482 - val_accuracy: 0.5371\n",
            "Epoch 2/50\n",
            "432/432 [==============================] - 762s 2s/step - loss: 1.1645 - accuracy: 0.5308 - val_loss: 0.9173 - val_accuracy: 0.6261\n",
            "Epoch 3/50\n",
            "432/432 [==============================] - 761s 2s/step - loss: 1.0017 - accuracy: 0.5979 - val_loss: 0.8154 - val_accuracy: 0.6646\n",
            "Epoch 4/50\n",
            "432/432 [==============================] - 759s 2s/step - loss: 0.8997 - accuracy: 0.6438 - val_loss: 0.7047 - val_accuracy: 0.7145\n",
            "Epoch 5/50\n",
            "432/432 [==============================] - 758s 2s/step - loss: 0.8362 - accuracy: 0.6722 - val_loss: 0.6573 - val_accuracy: 0.7528\n",
            "Epoch 6/50\n",
            "432/432 [==============================] - 760s 2s/step - loss: 0.7515 - accuracy: 0.7010 - val_loss: 0.6078 - val_accuracy: 0.7713\n",
            "Epoch 7/50\n",
            "432/432 [==============================] - 761s 2s/step - loss: 0.7433 - accuracy: 0.7086 - val_loss: 0.5886 - val_accuracy: 0.7849\n",
            "Epoch 8/50\n",
            "432/432 [==============================] - 760s 2s/step - loss: 0.7226 - accuracy: 0.7156 - val_loss: 0.5630 - val_accuracy: 0.7988\n",
            "Epoch 9/50\n",
            "432/432 [==============================] - 760s 2s/step - loss: 0.6636 - accuracy: 0.7359 - val_loss: 0.5288 - val_accuracy: 0.8142\n",
            "Epoch 10/50\n",
            "432/432 [==============================] - 758s 2s/step - loss: 0.6558 - accuracy: 0.7391 - val_loss: 0.5281 - val_accuracy: 0.7904\n",
            "Epoch 11/50\n",
            " 79/432 [====>.........................] - ETA: 9:27 - loss: 0.6463 - accuracy: 0.7429"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "images_train, images_val, num_features_train, num_features_val, labels_train, labels_val = train_test_split(images, numerical_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define early stopping criteria\n",
        "early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "\n",
        "# Define checkpoint to save model weights\n",
        "checkpoint = ModelCheckpoint(filepath='/content/drive/My Drive/model_weights_epoch_{epoch:02d}.h5', monitor='val_loss', save_best_only=False, save_weights_only=True, mode='auto', period=1)\n",
        "\n",
        "# Train your model with early stopping and model checkpoint\n",
        "history = model.fit(\n",
        "    [images_train, num_features_train],\n",
        "    labels_train,\n",
        "    batch_size=32,\n",
        "    epochs=50,\n",
        "    validation_data=([images_val, num_features_val], labels_val),\n",
        "    callbacks=[early_stopping_monitor, checkpoint]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOpjrw3mLQxM"
      },
      "outputs": [],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_Jq7Or-PNma"
      },
      "outputs": [],
      "source": [
        "model.save('/content/my_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAfcGvX-gopI"
      },
      "outputs": [],
      "source": [
        "for label in range(5):  # Assuming labels 0 to 4\n",
        "    folder_path = os.path.join(\"/content/drive/MyDrive/Baseline_DR\", str(label))\n",
        "    files = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "          files+=1\n",
        "    print(label)\n",
        "    print(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRCPjubTjG6I"
      },
      "outputs": [],
      "source": [
        "# Plotting the loss curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Iteration Curve')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn4AlUI2ucv0"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the validation data\n",
        "predictions = model.predict([validation_images, validation_numerical_features])\n",
        "\n",
        "# Calculate AUC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Assuming labels are binary (0 or 1)\n",
        "auc = roc_auc_score(validation_labels, predictions)\n",
        "\n",
        "print(\"AUC:\", auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQk5LHzFlXlA"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(validation_labels, predictions)\n",
        "\n",
        "# Calculate specificity and sensitivity\n",
        "specificity = 1 - fpr\n",
        "sensitivity = tpr\n",
        "\n",
        "# Plot specificity vs sensitivity curve\n",
        "plt.plot(specificity, sensitivity, marker='o', linestyle='-')\n",
        "plt.xlabel('Specificity')\n",
        "plt.ylabel('Sensitivity')\n",
        "plt.title('Specificity vs Sensitivity Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru2qG6GTpybK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}